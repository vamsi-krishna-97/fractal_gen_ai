{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6rdwlCsCGt"
      },
      "source": [
        "# Using Gemini 1.5 Flash with Python for diverse real-world tasks using Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTzBUFWQ-OWj"
      },
      "source": [
        "In this notebook you will use the Google's Gemini to solve:\n",
        "\n",
        "- Task 1: Zero-shot Classification\n",
        "- Task 2: Few-shot Classification\n",
        "- Task 3: Coding Tasks - Python\n",
        "- Task 4: Coding Tasks - SQL\n",
        "- Task 5: Information Extraction\n",
        "- Task 6: Closed-Domain Question Answering\n",
        "- Task 7: Open-Domain Question Answering\n",
        "- Task 8: Document Summarization\n",
        "- Task 9: Transformation\n",
        "- Task 10: Translation\n",
        "\n",
        "\n",
        "\n",
        "___Created By: Dipanjan (DJ)___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "## Install Google GenAI dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "01e1a1de-aa8c-4111-f456-b3f093636310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai==0.8.3 in /usr/local/lib/python3.12/dist-packages (0.8.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (2.182.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (2.11.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.3) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai==0.8.3) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai==0.8.3) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai==0.8.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai==0.8.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai==0.8.3) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai==0.8.3) (3.2.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai==0.8.3) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai==0.8.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwGjVWK4q6F"
      },
      "source": [
        "## Load Google GenAI API Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryheOZuXxa41",
        "outputId": "6296a243-6fd4-426a-8cea-f28d83bb38cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google Gemini API Key: 路路路路路路路路路路\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "gemini_key = getpass(\"Enter your Google Gemini API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=gemini_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWhgxCy5bA6"
      },
      "source": [
        "## Create Google Gemini Chat Completion Access Function\n",
        "\n",
        "This function will use the [Google Gemini API](https://ai.google.dev/tutorials/python_quickstart)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA9gVCwK0WKd"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gemini-2.5-flash\"):\n",
        "    model = genai.GenerativeModel(model,\n",
        "                                  generation_config=genai.GenerationConfig(\n",
        "                                      temperature=0,\n",
        "    ))\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFZjzuGjCOw"
      },
      "source": [
        "## Let's try out the Gemini API!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "KK-kjmMoi5rO",
        "outputId": "8cbbe451-5f5e-4a3e-cc03-5b77fbe9a29a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here are two bullet points explaining Generative AI:\n\n*   **Creates New Content:** Generative AI models are designed to produce novel, original content (like text, images, audio, or video) that didn't exist before, rather than just analyzing or classifying existing data.\n*   **Learns from Data Patterns:** It achieves this by learning complex patterns, structures, and styles from vast amounts of existing data, enabling it to generate outputs that are realistic, coherent, and often indistinguishable from human-made creations."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "response = get_completion(prompt='Explain Generative AI in 2 bullet points',\n",
        "                          model=\"gemini-2.5-flash\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeDkpvGDhMGV"
      },
      "source": [
        "## Task 1: Zero-shot Classification\n",
        "\n",
        "This prompt tests an LLM's text classification capabilities by prompting it to classify a piece of text without providing any examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRbBZB57hT0G"
      },
      "outputs": [],
      "source": [
        "reviews = [\n",
        "    f\"\"\"\n",
        "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
        "    The sound quality is impressively clear with just the right amount of bass.\n",
        "    It's also waterproof, which tested true during a recent splashing incident.\n",
        "    Though it's compact, the volume can really fill the space.\n",
        "    The price was a bargain for such high-quality sound.\n",
        "    Shipping was also on point, arriving two days early in secure packaging.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Needed a new kitchen blender, but this model has been a nightmare.\n",
        "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
        "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
        "    I thought the brand meant quality, but this product has proven me wrong.\n",
        "    Plus, it arrived three days late. Definitely not worth the expense.\n",
        "    \"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZwPaViatl7f"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "\n",
        "for review in reviews:\n",
        "  prompt = f\"\"\"\n",
        "              Act as a product review analyst.\n",
        "              Given the following review,\n",
        "              Display the overall sentiment for the review as only one of the following:\n",
        "              Positive, Negative OR Neutral\n",
        "\n",
        "              ```{review}```\n",
        "              \"\"\"\n",
        "  response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "  responses.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdUFkKAmtmBj",
        "outputId": "a56c4255-e574-4256-dda8-0a0afe55e74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: \n",
            "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
            "    The sound quality is impressively clear with just the right amount of bass.\n",
            "    It's also waterproof, which tested true during a recent splashing incident.\n",
            "    Though it's compact, the volume can really fill the space.\n",
            "    The price was a bargain for such high-quality sound.\n",
            "    Shipping was also on point, arriving two days early in secure packaging.\n",
            "    \n",
            "Sentiment: Positive\n",
            "------\n",
            "\n",
            "\n",
            "Review: \n",
            "    Needed a new kitchen blender, but this model has been a nightmare.\n",
            "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
            "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
            "    I thought the brand meant quality, but this product has proven me wrong.\n",
            "    Plus, it arrived three days late. Definitely not worth the expense.\n",
            "    \n",
            "Sentiment: Negative\n",
            "------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for review, response in zip(reviews, responses):\n",
        "  print('Review:', review)\n",
        "  print('Sentiment:', response)\n",
        "  print('------')\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WC6aycZ9Qe3"
      },
      "source": [
        "## Task 2: Few-shot Classification\n",
        "\n",
        "This prompt tests an LLM's text classification capabilities by prompting it to classify a piece of text by providing a few examples of inputs and outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQZdygfUoXGT"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "\n",
        "for review in reviews:\n",
        "  prompt = f\"\"\"\n",
        "              Act as a product review analyst.\n",
        "              Given the following review,\n",
        "              Display only the overall sentiment for the review:\n",
        "\n",
        "              Try to classify it by using the following examples as a reference:\n",
        "\n",
        "              Review: Just received the Laptop I ordered for work, and it's amazing.\n",
        "              Sentiment: \n",
        "\n",
        "              Review: Needed a new mechanical keyboard, but this model has been totally disappointing.\n",
        "              Sentiment: \n",
        "\n",
        "              Review: ```{review}```\n",
        "              \"\"\"\n",
        "  response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "  responses.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCsMzs3QodqR",
        "outputId": "e02270c4-61f6-4d05-b78f-663441deac11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: \n",
            "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
            "    The sound quality is impressively clear with just the right amount of bass.\n",
            "    It's also waterproof, which tested true during a recent splashing incident.\n",
            "    Though it's compact, the volume can really fill the space.\n",
            "    The price was a bargain for such high-quality sound.\n",
            "    Shipping was also on point, arriving two days early in secure packaging.\n",
            "    \n",
            "Sentiment: \n",
            "------\n",
            "\n",
            "\n",
            "Review: \n",
            "    Needed a new kitchen blender, but this model has been a nightmare.\n",
            "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
            "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
            "    I thought the brand meant quality, but this product has proven me wrong.\n",
            "    Plus, it arrived three days late. Definitely not worth the expense.\n",
            "    \n",
            "Sentiment: Sentiment: \n",
            "------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for review, response in zip(reviews, responses):\n",
        "  print('Review:', review)\n",
        "  print('Sentiment:', response)\n",
        "  print('------')\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEKTyfWf9cxY"
      },
      "source": [
        "## Task 3: Coding Tasks - Python\n",
        "\n",
        "This prompt tests an LLM's capabilities for generating python code based on various tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xawr-Co9b0t"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Act as an expert in generating python code\n",
        "\n",
        "Your task is to generate python code\n",
        "to build a Chain of Though prompt pattern using\n",
        "the openai python library\n",
        "\"\"\"\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Cenat0G9-Bw",
        "outputId": "d0be822e-51ee-4e8f-9b34-78e38da6df42"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As an expert in generating Python code for AI applications, I'll provide you with a robust and well-commented solution to implement the Chain of Thought (CoT) prompting pattern using the `openai` Python library.\n\nThe Chain of Thought pattern encourages the Language Model (LLM) to break down complex problems into intermediate reasoning steps before providing a final answer. This often leads to more accurate and reliable results, especially for tasks requiring logical deduction, arithmetic, or multi-step problem-solving.\n\n---\n\n### Key Concepts of Chain of Thought (CoT) Prompting:\n\n1.  **Explicit Instruction:** You explicitly tell the LLM to \"think step by step,\" \"explain your reasoning,\" or \"break down the problem.\"\n2.  **Intermediate Steps:** The LLM generates a series of logical steps that lead to the final answer.\n3.  **Final Answer:** After the reasoning, the LLM provides the ultimate solution.\n4.  **Zero-shot CoT:** Simply adding \"Let's think step by step\" to your prompt.\n5.  **Few-shot CoT:** Providing examples of input-reasoning-output pairs in your prompt. (This example will focus on Zero-shot CoT for simplicity, but the structure can be extended for few-shot).\n\n---\n\n### Python Code Implementation\n\nThis code will:\n*   Set up the OpenAI client.\n*   Define a function to generate a CoT response.\n*   Include a clear system message and a user message that incorporates the CoT instruction.\n*   Demonstrate how to parse the final answer from the CoT output (optional but useful).\n\n```python\nimport os\nimport openai\nimport re\nfrom typing import Dict, Any, Optional\n\n# --- Configuration ---\n# Ensure you have your OpenAI API key set as an environment variable.\n# For example: export OPENAI_API_KEY='your_api_key_here'\n# Or, if you prefer to set it directly (less secure for production):\n# openai.api_key = \"YOUR_ACTUAL_OPENAI_API_KEY\"\n\n# Initialize the OpenAI client\n# It will automatically pick up the OPENAI_API_KEY environment variable\ntry:\n    client = openai.OpenAI()\nexcept openai.OpenAIError as e:\n    print(f\"Error initializing OpenAI client: {e}\")\n    print(\"Please ensure your OPENAI_API_KEY environment variable is set.\")\n    exit(1)\n\n# --- Constants ---\nDEFAULT_MODEL = \"gpt-3.5-turbo\" # Or \"gpt-4\", \"gpt-4o\" for more advanced reasoning\nFINAL_ANSWER_TAG = \"<answer>\" # A tag to easily identify the final answer in the output\n\ndef generate_chain_of_thought_response(\n    problem_description: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.7,\n    max_tokens: int = 500,\n    system_message_content: str = \"You are an expert problem solver. Think step by step to solve complex problems and provide clear, concise answers.\"\n) -> Optional[str]:\n    \"\"\"\n    Generates a response using the Chain of Thought prompting pattern.\n\n    Args:\n        problem_description (str): The core problem or question to be solved.\n        model (str): The OpenAI model to use (e.g., \"gpt-3.5-turbo\", \"gpt-4\").\n        temperature (float): Controls the randomness of the output. Lower values (e.g., 0.2)\n                             make the output more deterministic, higher values (e.g., 0.8)\n                             make it more creative.\n        max_tokens (int): The maximum number of tokens to generate in the response.\n                          CoT responses can be longer, so adjust as needed.\n        system_message_content (str): The content for the system message, setting the AI's role.\n\n    Returns:\n        Optional[str]: The full response content from the LLM, including the reasoning steps\n                       and the final answer, or None if an error occurs.\n    \"\"\"\n    # Construct the user message with the Chain of Thought instruction\n    user_message_content = f\"\"\"\n    Problem: {problem_description}\n\n    Please think step by step. Explain your reasoning clearly for each step before providing the final answer.\n    After your detailed reasoning, provide the final answer enclosed in '{FINAL_ANSWER_TAG}' tags.\n\n    Example:\n    Reasoning Step 1: ...\n    Reasoning Step 2: ...\n    {FINAL_ANSWER_TAG}Your Final Answer Here{FINAL_ANSWER_TAG}\n    \"\"\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_message_content},\n        {\"role\": \"user\", \"content\": user_message_content},\n    ]\n\n    print(f\"\\n--- Sending Request to OpenAI ({model}) ---\")\n    print(f\"Problem: {problem_description}\")\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n        full_response_content = response.choices[0].message.content\n        return full_response_content\n\n    except openai.APIError as e:\n        print(f\"OpenAI API Error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return None\n\ndef extract_final_answer(response_text: str) -> Optional[str]:\n    \"\"\"\n    Extracts the final answer from the LLM's response using the predefined tag.\n\n    Args:\n        response_text (str): The full response string from the LLM.\n\n    Returns:\n        Optional[str]: The extracted final answer, or None if not found.\n    \"\"\"\n    # Use regex to find content between the FINAL_ANSWER_TAGs\n    match = re.search(rf\"{FINAL_ANSWER_TAG}(.*?){FINAL_ANSWER_TAG}\", response_text, re.DOTALL)\n    if match:\n        return match.group(1).strip()\n    return None\n\n# --- Example Usage ---\nif __name__ == \"__main__\":\n    # Example 1: A simple arithmetic problem\n    problem1 = \"If a train travels at 60 miles per hour for 2.5 hours, how far does it travel?\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXAMPLE 1: Simple Arithmetic Problem\")\n    print(\"=\"*80)\n    cot_response1 = generate_chain_of_thought_response(problem1)\n\n    if cot_response1:\n        print(\"\\n--- Full Chain of Thought Response ---\")\n        print(cot_response1)\n        final_answer1 = extract_final_answer(cot_response1)\n        if final_answer1:\n            print(f\"\\n--- Extracted Final Answer ---\")\n            print(final_answer1)\n        else:\n            print(\"\\n--- Could not extract final answer using the tag. ---\")\n    else:\n        print(\"Failed to get a response for Problem 1.\")\n\n    # Example 2: A more complex logical reasoning problem\n    problem2 = \"There are three boxes. One contains only apples, one contains only oranges, and one contains both apples and oranges. Each box is incorrectly labeled. You are allowed to open only one box, and without looking inside, take out one piece of fruit. By looking at the fruit, how can you correctly label all the boxes?\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXAMPLE 2: Complex Logical Reasoning Problem\")\n    print(\"=\"*80)\n    cot_response2 = generate_chain_of_thought_response(problem2, model=\"gpt-4o\", max_tokens=800) # Using a more capable model for complex tasks\n\n    if cot_response2:\n        print(\"\\n--- Full Chain of Thought Response ---\")\n        print(cot_response2)\n        final_answer2 = extract_final_answer(cot_response2)\n        if final_answer2:\n            print(f\"\\n--- Extracted Final Answer ---\")\n            print(final_answer2)\n        else:\n            print(\"\\n--- Could not extract final answer using the tag. ---\")\n    else:\n        print(\"Failed to get a response for Problem 2.\")\n\n    # Example 3: A problem where CoT helps avoid common pitfalls\n    problem3 = \"A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXAMPLE 3: Classic Riddle (CoT helps avoid intuitive wrong answer)\")\n    print(\"=\"*80)\n    cot_response3 = generate_chain_of_thought_response(problem3)\n\n    if cot_response3:\n        print(\"\\n--- Full Chain of Thought Response ---\")\n        print(cot_response3)\n        final_answer3 = extract_final_answer(cot_response3)\n        if final_answer3:\n            print(f\"\\n--- Extracted Final Answer ---\")\n            print(final_answer3)\n        else:\n            print(\"\\n--- Could not extract final answer using the tag. ---\")\n    else:\n        print(\"Failed to get a response for Problem 3.\")\n```\n\n---\n\n### How to Run This Code:\n\n1.  **Install the OpenAI Library:**\n    ```bash\n    pip install openai\n    ```\n2.  **Set Your OpenAI API Key:**\n    The most secure way is to set it as an environment variable:\n    *   **Linux/macOS:**\n        ```bash\n        export OPENAI_API_KEY='your_api_key_here'\n        ```\n    *   **Windows (Command Prompt):**\n        ```cmd\n        set OPENAI_API_KEY=your_api_key_here\n        ```\n    *   **Windows (PowerShell):**\n        ```powershell\n        $env:OPENAI_API_KEY='your_api_key_here'\n        ```\n    Replace `'your_api_key_here'` with your actual key from the [OpenAI dashboard](https://platform.openai.com/account/api-keys).\n    *Alternatively, you can uncomment and set `openai.api_key = \"YOUR_ACTUAL_OPENAI_API_KEY\"` directly in the code, but this is not recommended for production.*\n3.  **Save the Code:** Save the code above as a Python file (e.g., `cot_prompt.py`).\n4.  **Run from Terminal:**\n    ```bash\n    python cot_prompt.py\n    ```\n\n---\n\n### Explanation and Best Practices:\n\n1.  **`openai.OpenAI()` Client:** The modern way to interact with the OpenAI API. It automatically picks up the `OPENAI_API_KEY` environment variable.\n2.  **System Message:** `system_message_content` sets the overall persona and instructions for the AI. Here, it establishes the AI as an \"expert problem solver\" and reinforces the CoT approach.\n3.  **User Message Construction:**\n    *   The `problem_description` is embedded.\n    *   Crucially, the explicit instruction \"Please think step by step. Explain your reasoning clearly for each step before providing the final answer.\" is added. This is the core of the Zero-shot CoT pattern.\n    *   We also instruct the model to enclose the final answer in specific tags (`<answer>`). This is a powerful technique to make the output programmatically parsable.\n4.  **`client.chat.completions.create()`:** This is the API call to the chat completion endpoint.\n    *   `model`: Choose an appropriate model. `gpt-3.5-turbo` is cost-effective, while `gpt-4` or `gpt-4o` offer superior reasoning capabilities for more complex problems.\n    *   `messages`: A list of dictionaries representing the conversation history.\n    *   `temperature`: Controls creativity. For reasoning tasks, a lower temperature (e.g., 0.2-0.7) is often preferred to get more focused and deterministic answers.\n    *   `max_tokens`: CoT responses are typically longer than direct answers, so ensure `max_tokens` is sufficient.\n5.  **Error Handling:** Includes `try-except` blocks to catch potential `openai.APIError` (e.g., invalid key, rate limits) and other unexpected errors.\n6.  **`extract_final_answer()`:** This utility function uses regular expressions to find and extract the content within the `FINAL_ANSWER_TAG`s. This is extremely useful if you need to programmatically use only the final answer without the preceding reasoning.\n7.  **Examples:** The `if __name__ == \"__main__\":` block provides three distinct examples, demonstrating how CoT can improve results for arithmetic, logical puzzles, and problems known to trick human intuition.\n\nThis setup provides a solid foundation for building more sophisticated AI applications that leverage the power of Chain of Thought reasoning."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JoSF5wb_Imw"
      },
      "source": [
        "## Task 4: Coding Tasks - SQL\n",
        "\n",
        "This prompt tests an LLM's capabilities for generating SQL code based on various tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XKGZIG3_Tjl"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Act as an expert in generating SQL code.\n",
        "\n",
        "Understand the following schema of the database tables carefully:\n",
        "Table departments, columns = [DepartmentId, DepartmentName]\n",
        "Table employees, columns = [EmployeeId, EmployeeName, DepartmentId]\n",
        "Table salaries, columns = [EmployeeId, Salary]\n",
        "\n",
        "Create a MySQL query for the employee with max salary in the 'IT' Department.\n",
        "\"\"\"\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4cFqsBtSA7Wx",
        "outputId": "ac893b8f-c037-4aab-86ab-5c53117bc540"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```sql\nSELECT\n  e.EmployeeName,\n  s.Salary\nFROM\n  employees AS e\nJOIN\n  departments AS d\n  ON e.DepartmentId = d.DepartmentId\nJOIN\n  salaries AS s\n  ON e.EmployeeId = s.EmployeeId\nWHERE\n  d.DepartmentName = 'IT'\nORDER BY\n  s.Salary DESC\nLIMIT 1;\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mck_SlcM3bG"
      },
      "source": [
        "## Task 5: Information Extraction\n",
        "\n",
        "This prompt tests an LLM's capabilities for extracting and analyzing key entities from documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxciK2wsIw3U"
      },
      "outputs": [],
      "source": [
        "clinical_note = \"\"\"\n",
        "60-year-old man in NAD with a h/o CAD, DM2, asthma, pharyngitis, SBP,\n",
        "and HTN on altace for 8 years awoke from sleep around 1:00 am this morning\n",
        "with a sore throat and swelling of the tongue.\n",
        "He came immediately to the ED because he was having difficulty swallowing and\n",
        "some trouble breathing due to obstruction caused by the swelling.\n",
        "He did not have any associated SOB, chest pain, itching, or nausea.\n",
        "He has not noticed any rashes.\n",
        "He says that he feels like it is swollen down in his esophagus as well.\n",
        "He does not recall vomiting but says he might have retched a bit.\n",
        "In the ED he was given 25mg benadryl IV, 125 mg solumedrol IV,\n",
        "and pepcid 20 mg IV.\n",
        "Family history of CHF and esophageal cancer (father).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcazKzm0JGKc"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Act as an expert in analyzing and understanding clinical doctor notes in healthcare.\n",
        "Extract all symptoms only from the clinical note information below.\n",
        "Differentiate between symptoms that are present vs. absent.\n",
        "Give me the probability (high/ medium/ low) of how sure you are about the result.\n",
        "Add a note on the probabilities and why you think so.\n",
        "\n",
        "Output as a markdown table with the following columns,\n",
        "all symptoms should be expanded and no acronyms unless you don't know:\n",
        "\n",
        "Symptoms | Present/Denies | Probability.\n",
        "\n",
        "Also expand all acronyms.\n",
        "Output that also as a separate appendix table in Markdown.\n",
        "\n",
        "Clinical Note:\n",
        "```{clinical_note}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pqXCxmtEJl4h",
        "outputId": "97c0b80d-26c0-487b-bb86-1cf806ed99d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As an expert in analyzing clinical doctor notes, I have extracted all symptoms from the provided information, differentiating between those present and those denied by the patient. The probability of certainty for each finding is also provided, along with a detailed explanation.\n\n### Symptoms Analysis\n\n| Symptoms                      | Present/Denies | Probability |\n| :---------------------------- | :------------- | :---------- |\n| Sore throat                   | Present        | High        |\n| Swelling of the tongue        | Present        | High        |\n| Difficulty swallowing         | Present        | High        |\n| Trouble breathing             | Present        | High        |\n| Shortness of breath           | Denies         | High        |\n| Chest pain                    | Denies         | High        |\n| Itching                       | Denies         | High        |\n| Nausea                        | Denies         | High        |\n| Rashes                        | Denies         | High        |\n| Esophageal swelling sensation | Present        | Medium      |\n| Vomiting                      | Denies         | High        |\n| Retching                      | Present        | Medium      |\n\n---\n\n### Note on Probabilities\n\nThe probabilities are assigned based on the clarity and directness of the statements within the clinical note:\n\n*   **High Probability:** This is assigned when a symptom is explicitly stated as present or denied by the patient or is an unambiguous observation. For example, \"awoke from sleep... with a sore throat and swelling of the tongue\" or \"He did not have any associated SOB, chest pain, itching, or nausea\" are direct and clear statements.\n*   **Medium Probability:** This is used when the patient's statement includes a slight degree of uncertainty or describes a subjective sensation that is inherently less objective. For instance, \"He says that he feels like it is swollen down in his esophagus as well\" is a subjective feeling, and \"He does not recall vomiting but says he might have retched a bit\" indicates some uncertainty in the patient's recall regarding the act of retching. There are no \"low probability\" assignments in this note as all statements, even with slight uncertainty, are relatively clear in their intent.\n\n---\n\n### Appendix: Acronyms Expansion\n\n| Acronym | Expansion                     |\n| :------ | :---------------------------- |\n| NAD     | No Acute Distress             |\n| h/o     | History of                    |\n| CAD     | Coronary Artery Disease       |\n| DM2     | Diabetes Mellitus Type 2      |\n| SBP     | Spontaneous Bacterial Peritonitis (as a historical medical condition) |\n| HTN     | Hypertension                  |\n| ED      | Emergency Department          |\n| SOB     | Shortness of Breath           |\n| IV      | Intravenous                   |\n| CHF     | Congestive Heart Failure      |"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jucY6L9w14Z"
      },
      "source": [
        "## Task 6: Closed-Domain Question Answering\n",
        "\n",
        "Question Answering (QA) is a natural language processing task which involves generating the desired answer for the given question. Question Answering can be open-domain QA or closed-domain QA depending on whether the LLM is provided with the relevant context or not.\n",
        "\n",
        "In the case of closed-domain QA, a question along with relevant context is given. Here the context is nothing but the relevant text which ideally should have the answer. Just like a RAG workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I7ZsP3OxL-n"
      },
      "outputs": [],
      "source": [
        "report = \"\"\"\n",
        "Three quarters (77%) of the population saw an increase in their regular outgoings over the past year,\n",
        "according to findings from our recent consumer survey. In contrast, just over half (54%) of respondents\n",
        "had an increase in their salary, which suggests that the burden of costs outweighing income remains for\n",
        "most. In total, across the 2,500 people surveyed, the increase in outgoings was 18%, three times higher\n",
        "than the 6% increase in income.\n",
        "\n",
        "Despite this, the findings of our survey suggest we have reached a plateau. Looking at savings,\n",
        "for example, the share of people who expect to make regular savings this year is just over 70%,\n",
        "broadly similar to last year. Over half of those saving plan to use some of the funds for residential\n",
        "property. A third are saving for a deposit, and a further 20% for an investment property or second home.\n",
        "\n",
        "But for some, their plans are being pushed back. 9% of respondents stated they had planned to purchase\n",
        "a new home this year but have now changed their mind. While for many the deposit may be an issue,\n",
        "the other driving factor remains the cost of the mortgage, which has been steadily rising the last\n",
        "few years. For those that currently own a property, the survey showed that in the last year,\n",
        "the average mortgage payment has increased from 拢668.51 to 拢748.94, or 12%.\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAPf8hD2xATM"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "How much has the average mortage payment increased in the last year?\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Using the following context information below please answer the following question\n",
        "to the best of your ability\n",
        "Context:\n",
        "{report}\n",
        "Question:\n",
        "{question}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "whY2mBoXxUPM",
        "outputId": "cc387c69-b415-4433-8cad-0ef8c63ac6d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The average mortgage payment has increased from 拢668.51 to 拢748.94, which is an increase of 12%."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "A-Dxn9d_xibD",
        "outputId": "f0263505-763f-4da7-868f-43e3f90450d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "54"
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"\"\"\n",
        "What percentage of people had an increase in salary last year? Show the answer just as a number.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Using the following context information below please answer the following question\n",
        "to the best of your ability\n",
        "Context:\n",
        "{report}\n",
        "Question:\n",
        "{question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuQlsSmWxyCd"
      },
      "source": [
        "## Task 7: Open-Domain Question Answering\n",
        "\n",
        "Question Answering (QA) is a natural language processing task which involves generating the desired answer for the given question.\n",
        "\n",
        "In the case of open-domain QA, only the question is asked without providing any context or information. Here, the LLM answers the question using the knowledge gained from large volumes of text data during its training. This is basically Zero-Shot QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV2MP4OdyETp"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Please answer the following question to the best of your ability\n",
        "Question:\n",
        "What is LangChain?\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OSC-IhDmyRYg",
        "outputId": "70f87a65-a689-4928-ddbb-9a9d3b710cd6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LangChain is an **open-source development framework** designed to simplify the creation of applications powered by large language models (LLMs). It provides a structured way to combine LLMs with other components, enabling developers to build more complex, context-aware, and interactive applications than would be possible with LLMs alone.\n\nThink of it as a **toolkit or an orchestration layer** that helps you connect LLMs to external data, computation, and other tools, allowing them to perform more sophisticated tasks.\n\nHere's a breakdown of what LangChain is and why it's important:\n\n1.  **Problem it Solves:**\n    *   LLMs are powerful, but often need more than just a single prompt.\n    *   They need to remember past interactions (memory).\n    *   They need to access up-to-date or proprietary information (retrieval/external data).\n    *   They need to perform actions (e.g., search the web, run code, call APIs).\n    *   Connecting these disparate components can be complex and boilerplate-heavy. LangChain abstracts much of this complexity.\n\n2.  **Core Philosophy:**\n    *   **Composability:** Break down complex LLM tasks into smaller, reusable components.\n    *   **Context-Awareness:** Enable LLMs to understand and maintain context over time and across interactions.\n    *   **Reasoning:** Allow LLMs to reason and make decisions about which tools to use and in what order.\n\n3.  **Key Modules/Components:**\n    LangChain is built around several core abstractions that make it powerful:\n\n    *   **Models:** Interfaces for interacting with various LLM providers (e.g., OpenAI, Hugging Face, Anthropic) and different model types (text, chat, embeddings).\n    *   **Prompts:** Tools for managing, optimizing, and generating prompts, including prompt templates, output parsers, and example selectors.\n    *   **Chains:** Sequences of calls to LLMs or other utilities. They allow you to combine multiple components into a single, coherent workflow (e.g., an `LLMChain` for a single LLM call, or a `SequentialChain` for multiple steps).\n    *   **Agents:** LLMs that can decide which tools to use and in what order to achieve a goal. Agents are the \"brain\" that can dynamically interact with the environment.\n    *   **Tools:** Functions that agents can call to interact with the outside world (e.g., a search engine, a calculator, an API call, a database query).\n    *   **Memory:** Mechanisms to persist state between calls of a chain or agent, allowing the LLM to remember past interactions and maintain conversational context.\n    *   **Retrieval:** Components for connecting LLMs to external data sources (e.g., vector databases, document loaders) to perform tasks like Retrieval Augmented Generation (RAG).\n\n4.  **What You Can Build with LangChain:**\n    *   **Advanced Chatbots:** Chatbots that remember past conversations, answer questions based on custom data, and perform actions.\n    *   **Question Answering over Documents:** Systems that can answer questions by searching and synthesizing information from large collections of documents (e.g., PDFs, internal wikis).\n    *   **Autonomous Agents:** LLMs that can break down complex tasks into sub-tasks, use various tools, and execute multi-step plans.\n    *   **Data Analysis Tools:** LLMs that can interact with databases or spreadsheets to extract insights.\n    *   **Summarization and Extraction Tools:** More sophisticated summarization or information extraction pipelines.\n\n5.  **Languages:**\n    LangChain is primarily available in Python and JavaScript/TypeScript.\n\nIn essence, LangChain empowers developers to move beyond simple LLM API calls and build sophisticated, intelligent applications that leverage the full potential of large language models by integrating them seamlessly with other data sources and computational tools."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMX1CcsdMg_C"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Please answer the following question to the best of your ability\n",
        "Question:\n",
        "What is LangGraph?\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8-AxYD5AMjk6",
        "outputId": "2b9356b8-a91d-4137-fd6a-e3a1e8b7b7d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**LangGraph** is a library built on top of LangChain that allows you to build **stateful, multi-actor applications** with LLMs by representing the application logic as a **graph**.\n\nThink of it as a more powerful and flexible way to orchestrate complex LLM workflows, especially those that require:\n\n1.  **Complex Control Flow:** Beyond simple linear chains.\n2.  **State Management:** Maintaining and updating information across multiple steps.\n3.  **Cycles and Loops:** Enabling iterative processes, self-correction, and agentic behavior.\n4.  **Multi-Agent Systems:** Coordinating multiple \"actors\" (LLMs, tools, human input) that interact with each other.\n\nHere's a breakdown of its core concepts and why it's powerful:\n\n### Core Concepts\n\n1.  **Nodes:**\n    *   These are the individual steps or actions in your workflow.\n    *   A node can be an LLM call, a tool invocation, a custom Python function, or even another LangChain Runnable.\n    *   Each node takes the current state as input and returns an update to that state.\n\n2.  **Edges:**\n    *   These define the transitions between nodes.\n    *   **Direct Edges:** Simply move from one node to another unconditionally.\n    *   **Conditional Edges:** Allow the graph to branch based on the current state or the output of a node. This is crucial for dynamic decision-making.\n\n3.  **State:**\n    *   LangGraph manages a shared \"state\" object (typically a dictionary) that is passed between nodes.\n    *   Each node can read from and write to this state, allowing information to persist and evolve throughout the graph's execution. This is what makes it \"stateful.\"\n\n4.  **Cycles:**\n    *   Unlike traditional Directed Acyclic Graphs (DAGs), LangGraph explicitly supports cycles. This is a key differentiator.\n    *   Cycles enable powerful patterns like:\n        *   **Self-correction:** An agent tries an action, observes the result, and if it's not satisfactory, loops back to try again or refine its approach.\n        *   **Iterative refinement:** Generating an output, evaluating it, and then improving it in subsequent steps.\n        *   **Agentic behavior:** The classic \"plan, act, observe, reflect\" loop.\n\n### Why Use LangGraph?\n\n*   **Orchestration of Complex Workflows:** When your LLM application needs more than a simple sequence of steps, LangGraph provides the structure to manage intricate logic, branching, and looping.\n*   **Building Autonomous Agents:** It's ideal for creating sophisticated agents that can reason, use tools, and self-correct over multiple turns.\n*   **Multi-Agent Systems:** You can define different \"agents\" (e.g., a researcher agent, a coding agent, a critic agent) as nodes or sub-graphs and have them interact and pass information through the shared state.\n*   **Human-in-the-Loop:** Easily integrate points where human input or approval is required before proceeding.\n*   **Introspection and Debugging:** The graph structure makes it easier to visualize the flow of your application and understand where issues might arise.\n*   **Leverages LangChain's Ecosystem:** It seamlessly integrates with all LangChain components (LLMs, tools, retrievers, runnables).\n\n### Analogy\n\nImagine building a complex Rube Goldberg machine.\n*   **LangChain Runnables** are like individual mechanisms (a lever, a ramp, a pulley).\n*   **LangChain Chains** are like a simple, linear sequence of these mechanisms.\n*   **LangGraph** is like the blueprint for a Rube Goldberg machine that can make decisions, retry steps if something fails, or even have different parts of the machine work in parallel and then converge, all while keeping track of the ball's position and momentum (the state).\n\nIn essence, LangGraph provides the robust control flow and state management necessary to move beyond simple LLM interactions and build truly dynamic, intelligent, and resilient AI applications."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "REutW_DWauKF",
        "outputId": "e6115ad2-e08f-4137-cce0-94d36ac233d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LangGraph is a library built on top of LangChain that is designed for building **stateful, multi-actor applications with LLMs** by representing the application's logic as a **computational graph**.\n\nHere's a breakdown of what that means:\n\n1.  **Stateful:** Unlike simple sequential chains, LangGraph allows you to define and manage a shared \"state\" that persists and evolves across multiple steps and interactions within your application. Each node in the graph can read and update this state.\n\n2.  **Multi-Actor:** It's particularly well-suited for scenarios where multiple \"actors\" (which could be different LLMs, tools, human users, or custom code) need to interact and make decisions in a coordinated way.\n\n3.  **Computational Graph:**\n    *   **Nodes:** These are the individual steps or components of your application. A node could be:\n        *   Calling an LLM.\n        *   Using a tool (e.g., searching the web, calling an API).\n        *   Applying custom Python logic.\n        *   Receiving human input.\n    *   **Edges:** These define the flow between nodes. Edges can be:\n        *   **Conditional:** The next node to execute depends on the output of the current node or the current state. This allows for complex decision-making, branching, and self-correction.\n        *   **Unconditional:** Always move from one node to another.\n    *   **Cycles (Loops):** A key feature is the ability to create cycles in the graph. This is crucial for agentic behavior where an LLM might need to:\n        *   Try a tool, observe the result, and then decide to try another tool or rephrase its query.\n        *   Loop through a process until a specific condition is met (e.g., \"keep searching until I find a relevant document\").\n        *   Engage in a multi-turn conversation or reasoning process.\n\n**Why was LangGraph created?**\n\nWhile LangChain provides powerful tools for building LLM applications, complex agentic workflows often require more sophisticated orchestration than simple sequential chains. LangGraph addresses this by providing a framework that enables:\n\n*   **Advanced Agentic Behavior:** Building agents that can reason over multiple steps, use tools, self-correct, and engage in complex decision-making.\n*   **Human-in-the-Loop:** Easily integrating points where human feedback or intervention is required.\n*   **Robustness and Self-Correction:** The ability to define fallback paths or retry mechanisms within the graph.\n*   **Clearer Logic:** Visualizing the flow of a complex application as a graph can make it easier to understand, debug, and maintain.\n\nIn essence, LangGraph provides a powerful and flexible way to design and implement sophisticated, stateful, and highly interactive LLM applications, especially those involving autonomous agents."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Please answer the following question to the best of your ability\n",
        "Question:\n",
        "What is LangGraph?\n",
        "If you are not able to find the answer in your trained knowledge just say you don't know\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnVmXCZAzyux"
      },
      "source": [
        "## Task 8: Document Summarization\n",
        "\n",
        "Document summarization is a natural language processing task which involves creating a concise summary of the given text, while still capturing all the important information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY53sXf_ymwE"
      },
      "outputs": [],
      "source": [
        "doc = \"\"\"\n",
        "Coronaviruses are a large family of viruses which may cause illness in animals or humans.\n",
        "In humans, several coronaviruses are known to cause respiratory infections ranging from the\n",
        "common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS) and Severe Acute Respiratory Syndrome (SARS).\n",
        "The most recently discovered coronavirus causes coronavirus disease COVID-19.\n",
        "COVID-19 is the infectious disease caused by the most recently discovered coronavirus.\n",
        "This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019.\n",
        "COVID-19 is now a pandemic affecting many countries globally.\n",
        "The most common symptoms of COVID-19 are fever, dry cough, and tiredness.\n",
        "Other symptoms that are less common and may affect some patients include aches\n",
        "and pains, nasal congestion, headache, conjunctivitis, sore throat, diarrhea,\n",
        "loss of taste or smell or a rash on skin or discoloration of fingers or toes.\n",
        "These symptoms are usually mild and begin gradually.\n",
        "Some people become infected but only have very mild symptoms.\n",
        "Most people (about 80%) recover from the disease without needing hospital treatment.\n",
        "Around 1 out of every 5 people who gets COVID-19 becomes seriously ill and develops difficulty breathing.\n",
        "Older people, and those with underlying medical problems like high blood pressure, heart and lung problems,\n",
        "diabetes, or cancer, are at higher risk of developing serious illness.\n",
        "However, anyone can catch COVID-19 and become seriously ill.\n",
        "People of all ages who experience fever and/or  cough associated with difficulty breathing/shortness of breath,\n",
        "chest pain/pressure, or loss of speech or movement should seek medical attention immediately.\n",
        "If possible, it is recommended to call the health care provider or facility first,\n",
        "so the patient can be directed to the right clinic.\n",
        "People can catch COVID-19 from others who have the virus.\n",
        "The disease spreads primarily from person to person through small droplets from the nose or mouth,\n",
        "which are expelled when a person with COVID-19 coughs, sneezes, or speaks.\n",
        "These droplets are relatively heavy, do not travel far and quickly sink to the ground.\n",
        "People can catch COVID-19 if they breathe in these droplets from a person infected with the virus.\n",
        "This is why it is important to stay at least 1 meter) away from others.\n",
        "These droplets can land on objects and surfaces around the person such as tables, doorknobs and handrails.\n",
        "People can become infected by touching these objects or surfaces, then touching their eyes, nose or mouth.\n",
        "This is why it is important to wash your hands regularly with soap and water or clean with alcohol-based hand rub.\n",
        "Practicing hand and respiratory hygiene is important at ALL times and is the best way to protect others and yourself.\n",
        "When possible maintain at least a 1 meter distance between yourself and others.\n",
        "This is especially important if you are standing by someone who is coughing or sneezing.\n",
        "Since some infected persons may not yet be exhibiting symptoms or their symptoms may be mild,\n",
        "maintaining a physical distance with everyone is a good idea if you are in an area where COVID-19 is circulating.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7upxVwh5y8te",
        "outputId": "f1b635f5-edb2-466d-e590-24304e7a2811"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nCoronaviruses are a family of viruses causing respiratory illnesses, ranging from the common cold to severe diseases like MERS and SARS, with the most recent being COVID-19. COVID-19, caused by a newly discovered coronavirus, emerged in Wuhan, China, in December 2019 and is now a global pandemic. Common symptoms include fever, dry cough, and tiredness, though severe cases with difficulty breathing can occur, especially in older individuals or those with underlying medical conditions. The disease spreads primarily through respiratory droplets from infected persons and by touching contaminated surfaces. Prevention involves maintaining physical distance (at least 1 meter), frequent hand washing, and practicing respiratory hygiene."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "You are an expert in generating accurate document summaries.\n",
        "Generate a summary of the given document.\n",
        "\n",
        "Document:\n",
        "{doc}\n",
        "\n",
        "\n",
        "Constraints: Please start the summary with the delimiter 'Summary'\n",
        "and limit the summary to 5 lines\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keC5k5S-18cw"
      },
      "source": [
        "## Task 9: Transformation\n",
        "\n",
        "You can use LLMs to take an existing document and transform it into other formats of content and even generate training data for fine-tuning or training models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta182I2t2FD0"
      },
      "outputs": [],
      "source": [
        "fact_sheet_mobile = \"\"\"\n",
        "PRODUCT NAME\n",
        "Samsung Galaxy Z Fold4 5G Black\n",
        "\n",
        "PRODUCT OVERVIEW\n",
        "Stands out. Stands up. Unfolds.\n",
        "The Galaxy Z Fold4 does a lot in one hand with its 15.73 cm(6.2-inch) Cover Screen.\n",
        "Unfolded, the 19.21 cm(7.6-inch) Main Screen lets you really get into the zone.\n",
        "Pushed-back bezels and the Under Display Camera means there's more screen\n",
        "and no black dot getting between you and the breathtaking Infinity Flex Display.\n",
        "Do more than more with Multi View. Whether toggling between texts or catching up\n",
        "on emails, take full advantage of the expansive Main Screen with Multi View.\n",
        "PC-like power thanks to Qualcomm Snapdragon 8+ Gen 1 processor in your pocket,\n",
        "transforms apps optimized with One UI to give you menus and more in a glance\n",
        "New Taskbar for PC-like multitasking. Wipe out tasks in fewer taps. Add\n",
        "apps to the Taskbar for quick navigation and bouncing between windows when\n",
        "you're in the groove.4 And with App Pair, one tap launches up to three apps,\n",
        "all sharing one super-productive screen\n",
        "Our toughest Samsung Galaxy foldables ever. From the inside out,\n",
        "Galaxy Z Fold4 is made with materials that are not only stunning,\n",
        "but stand up to life's bumps and fumbles. The front and rear panels,\n",
        "made with exclusive Corning Gorilla Glass Victus+, are ready to resist\n",
        "sneaky scrapes and scratches. With our toughest aluminum frame made with\n",
        "Armor Aluminum, this is one durable smartphone.\n",
        "Worlds first water resistant foldable smartphones. Be adventurous, rain\n",
        "or shine. You don't have to sweat the forecast when you've got one of the\n",
        "world's first water-resistant foldable smartphones.\n",
        "\n",
        "PRODUCT SPECS\n",
        "OS - Android 12.0\n",
        "RAM - 12 GB\n",
        "Product Dimensions - 15.5 x 13 x 0.6 cm; 263 Grams\n",
        "Batteries - 2 Lithium Ion batteries required. (included)\n",
        "Item model number - SM-F936BZKDINU_5\n",
        "Wireless communication technologies - Cellular\n",
        "Connectivity technologies - Bluetooth, Wi-Fi, USB, NFC\n",
        "GPS - True\n",
        "Special features - Fast Charging Support, Dual SIM, Wireless Charging, Built-In GPS, Water Resistant\n",
        "Other display features - Wireless\n",
        "Device interface - primary - Touchscreen\n",
        "Resolution - 2176x1812\n",
        "Other camera features - Rear, Front\n",
        "Form factor - Foldable Screen\n",
        "Colour - Phantom Black\n",
        "Battery Power Rating - 4400\n",
        "Whats in the box - SIM Tray Ejector, USB Cable\n",
        "Manufacturer - Samsung India pvt Ltd\n",
        "Country of Origin - China\n",
        "Item Weight - 263 g\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zko53u-N2KmD",
        "outputId": "9f643008-2998-423a-acae-6b70a06d111c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is a list of frequently asked questions (FAQ) based on the provided product description:\n\n**1. What are the screen sizes of the Samsung Galaxy Z Fold4?**\n**Answer:** The Galaxy Z Fold4 features a 15.73 cm (6.2-inch) Cover Screen for one-handed use and a 19.21 cm (7.6-inch) Main Screen when unfolded, offering an immersive Infinity Flex Display with an Under Display Camera.\n\n**2. What processor powers the Samsung Galaxy Z Fold4?**\n**Answer:** The device is powered by a PC-like Qualcomm Snapdragon 8+ Gen 1 processor, optimizing apps with One UI for quick glances.\n\n**3. What multitasking capabilities does the Galaxy Z Fold4 offer?**\n**Answer:** It provides Multi View for utilizing the expansive Main Screen, a new Taskbar for PC-like multitasking and quick navigation, and App Pair to launch up to three apps simultaneously on one screen.\n\n**4. How durable is the Samsung Galaxy Z Fold4?**\n**Answer:** The Galaxy Z Fold4 is built with exclusive Corning Gorilla Glass Victus+ on the front and rear panels for scratch resistance, and features an Armor Aluminum frame, making it one of Samsung's toughest foldables.\n\n**5. Is the Samsung Galaxy Z Fold4 water resistant?**\n**Answer:** Yes, it is one of the worlds first water-resistant foldable smartphones, designed to withstand splashes and rain.\n\n**6. What operating system and RAM does the Galaxy Z Fold4 use?**\n**Answer:** It runs on Android 12.0 and comes with 12 GB of RAM.\n\n**7. What are some of the special features of the Galaxy Z Fold4?**\n**Answer:** Key special features include Fast Charging Support, Dual SIM capability, Wireless Charging, Built-In GPS, and Water Resistance.\n\n**8. What is the battery capacity of the Samsung Galaxy Z Fold4?**\n**Answer:** The Galaxy Z Fold4 has a battery power rating of 4400 mAh and includes 2 Lithium Ion batteries."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt =f\"\"\"Turn the following product description into a list of frequently asked questions (FAQ).\n",
        "Show both the question and it's corresponding answer.\n",
        "Create at the max 8 FAQs\n",
        "\n",
        "Product description:\n",
        "```{fact_sheet_mobile}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53QdnJ0M3Vee"
      },
      "source": [
        "## Task 10: Translation\n",
        "\n",
        "You can use LLMs to take an existing document and translate it from a source to target language. You can also translate to multiple languages at the same time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "g-jdFJqJ2cWu",
        "outputId": "acf658ed-8d09-4322-b15c-77defee9aad4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**German:**\nHallo, wie geht es Ihnen heute?\n\n**Spanish:**\nHola, 驴c贸mo est谩 usted hoy?"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"You are an expert translator.\n",
        "Translate the given text from English to German and Spanish.\n",
        "\n",
        "Text: 'Hello, how are you today?'\n",
        "Translation:\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt, model='gemini-2.5-flash')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEZbrv698ASV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J-rtQea8ASW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCtPZTcp8ASX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApWfSq068ASX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4y8OOvz8ASX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67k2llej8ASY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z9ok_R98ASY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px9aAjNH8ASY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVmCQEs08ASZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABtF6Dpa8ASb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qcx_1kp8ASc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdqHQd1A8ASd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}